# Gamifier
CS1470 Final Project
# Introduction
Inspired by PolyGen (https://github.com/deepmind/deepmind-research/tree/master/polygen), a generative model for 3D meshes, we aim to simplify the game design process through implementing the model that takes in names of specific object classes and outputs the objects in 3D Mesh format, and then channeling the generative model to a prior Natural Language processing model to reconstruct a scene from text description. As described, our project is twofold. We plan to first implement the autoregressive generative model described in PolyGen, which takes in names of object classes and outputs 3D meshes of the objects. Then, tentatively, we will implement a language processing model that takes in a paragraph, understands the spatial relationship among different objects mentioned, and outputs the inferred positions of the objects in the scene as 3D coordinates. Putting the two models together enables us to use PolyGen outputs to reconstruct a scene that is commonly used in game design.

# Related Works
Polygon meshes are an efficient representation of 3D geometry. However, most of the existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using other representations such as voxels and point clouds. PolyGen: An Autoregressive Generative Model of 3D Meshes proposes an approach that models 3D meshes directly, predicting a mesh vertices and faces sequentially using a Transformer-based architecture, conditioning on a range of inputs, for example, object classes. The PolyGen model consists of two parts: A vertex model, that unconditionally models mesh vertices, and a face model, that models the mesh faces conditioned on input vertices. Both components make use of the Transformer architecture to express a distribution over the vertex sequences. PolyGen is capable of generating coherent and diverse mesh objects, and will be of central importance to our project, where we aim to reconstruct a scene using the PolyGen-generated samples.

Given that we are able to map names of object classes to generated 3D meshes using PolyGen, if we can also learn the spatial relationships among the objects, we would be able to reconstruct a scene that consists of multiple objects. To achieve this, we tentatively follow the approach outlined in Decoding Language Spatial Relations to 2D Spatial Arrangements (https://aclanthology.org/2020.findings-emnlp.408/), which trains a Spatial-Reasoning BERT model to decode text to 2D spatial arrangements in a non-autoregressive manner. We will modify the model to output 3D instead of 2D coordinates.

# Data
Since our project involves implementing existing papers, we will first train the models on the original datasets used in their corresponding papers, then compare model performance trained on alternative datasets, if any. Specifically, we will be using the data provided from ShapeNet (https://shapenet.org; https://arxiv.org/abs/1512.03012), a richly-annotated dataset for 3D models, in order to train our PolyGen model.

# Methodology
The PolyGen model consists of two parts: A vertex model, that unconditionally models mesh vertices, and a face model, that models the mesh faces conditioned on input vertices. Both components make use of the Transformer architecture (https://arxiv.org/abs/1706.03762) to express a distribution over the vertex sequences. The model will be trained on inputs from the shapenet dataset. In order to train the model efficiently — i.e. to reduce to space complexity of the training functions and preprocessing steps — we will not use meshes containing over 800 vertices or 2800 face indices when training; they will be ‘filtered out’ during the preprocessing steps of the model. Generally speaking, we will follow the same steps as described in PolyGen.

# Metrics
In order to test the success of our model we will compare the distribution of our model’s outputs to the shapenet test set. PolyGen divided the data in the following manner: 92.5% training data; 2.5% validation; and, 5% testing data. Our model will follow a similar testing suite through dividing the dataset from shapenet accordingly. The notion of an “accuracy score” will apply to our model due to the direct comparison nature of our model’s test and training data.

# Ethics
Recent innovations in virtual and augmented reality — such as the development of the Metaverse (https://about.facebook.com/meta/) — are suggestive of the pathway that many of the world’s leading technology companies, social networks, and other technological entities are leading into. Hence, gaining an understanding of the way a neural network models and predicts three-dimensional objects provides insight into how the worlds we may all be living and participating in could be designed and modeled in the future.

Additionally, the ethical implications of such a model can be extended to biases within the model; one needs to ensure that for a given stimulus the model has an equal probability of producing a random 3-dimensional object that is based off the input, but unrelated to external factors. For instance, if the stimulus is an “instrument”, the model should have an equal probability of producing a western instrument vs. an instrument from Zimbabwe.

Furthermore, deep learning is a good approach to this problem because 3D models and scenes are difficult to generate manually, and game designers typically rely on existing procedures and datasets, which can be learned by neural networks, given proper representations that are compatible with neural architectures and training approaches. Adopting the Deep Learning approach will significantly reduce the cost of game development by switching a large portion of the technical procedures of generating 3D shapes and scenes to pre-trained models, which in turn simplifies the game design process.
