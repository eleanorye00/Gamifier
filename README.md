# Gamifier: Deep 3D Mesh Generation for Game Design
_This is a tentative outline for CS1470 Deep Learning final project._

# Introduction
We aim to simplify the game design process with the Gamifier model, which combines relevant features of PolyGen and BERT to generate 3D scenes as meshes from text descriptions. We will implement PolyGen (https://github.com/deepmind/deepmind-research/tree/master/polygen), an autoregressive model that takes in names of specific object classes and outputs the objects in 3D Mesh format, and channel it to a Natural Language Processing model, which understands the spatial relationship among objects from texts and outputs the inferred positions of the objects in the scene as 3D coordinates. Combining these two models, we nicknamed the new umbrella model Gamifier, as we believe it will simplify the game design process and unlock a range of potential applications in 3D scene creation, augmented reality, and virtual reality.

# Related Works
Polygon meshes are an efficient representation of 3D geometry. However, most of the existing learning-based approaches have avoided the challenges of working with 3D meshes, instead using other representations such as voxels and point clouds. PolyGen: An Autoregressive Generative Model of 3D Meshes proposes an approach that models 3D meshes directly, predicting a mesh vertices and faces sequentially using a Transformer-based architecture, conditioning on a range of inputs, for example, object classes. The PolyGen model consists of two parts: A vertex model, that unconditionally models mesh vertices, and a face model, that models the mesh faces conditioned on input vertices. Both components make use of the Transformer architecture to express a distribution over the vertex sequences. PolyGen is capable of generating coherent and diverse mesh objects, and will be of central importance to our project, where we aim to reconstruct a scene using the PolyGen-generated samples.

Given that we are able to map names of object classes to generated 3D meshes using PolyGen, if we can also learn the spatial relationships among the objects, we would be able to reconstruct a scene that consists of multiple objects. To achieve this, we tentatively follow the approach outlined in Decoding Language Spatial Relations to 2D Spatial Arrangements (https://aclanthology.org/2020.findings-emnlp.408/), which trains a Spatial-Reasoning BERT model to decode text to 2D spatial arrangements in a non-autoregressive manner. We will modify the model to output 3D instead of 2D coordinates.

# Data
Since our project involves implementing existing papers, we will first train the models on the original datasets used in their corresponding papers, then compare model performance trained on alternative datasets, if any. Specifically, we will be using the data provided from ShapeNet (https://shapenet.org; https://arxiv.org/abs/1512.03012), a richly-annotated dataset for 3D models, in order to train our PolyGen model.

# Methodology
The PolyGen model consists of two parts: A vertex model, that unconditionally models mesh vertices, and a face model, that models the mesh faces conditioned on input vertices. Both components make use of the Transformer architecture (https://arxiv.org/abs/1706.03762) to express a distribution over the vertex sequences. The model will be trained on inputs from the shapenet dataset. In order to train the model efficiently — i.e. to reduce to space complexity of the training functions and preprocessing steps — we will not use meshes containing over 800 vertices or 2800 face indices when training; they will be ‘filtered out’ during the preprocessing steps of the model. Generally speaking, we will follow the same steps as described in PolyGen.

# Metrics
In order to test the success of our model we will compare the distribution of our model’s outputs to the shapenet test set. PolyGen divided the data in the following manner: 92.5% training data; 2.5% validation; and, 5% testing data. Our model will follow a similar testing suite through dividing the dataset from shapenet accordingly. The notion of an “accuracy score” will apply to our model due to the direct comparison nature of our model’s test and training data.

# Ethics
Recent innovations in virtual and augmented reality — such as the development of the Metaverse (https://about.facebook.com/meta/) — are suggestive of the pathway that many of the world’s leading technology companies, social networks, and other technological entities are leading into. Hence, gaining an understanding of the way a neural network models and predicts three-dimensional objects provides insight into how the worlds we may all be living and participating in could be designed and modeled in the future.

Additionally, the ethical implications of such a model can be extended to biases within the model; one needs to ensure that for a given stimulus the model has an equal probability of producing a random 3-dimensional object that is based off the input, but unrelated to external factors. For instance, if the stimulus is an “instrument”, the model should have an equal probability of producing a western instrument vs. an instrument from Zimbabwe.

Furthermore, deep learning is a good approach to this problem because 3D models and scenes are difficult to generate manually, and game designers typically rely on existing procedures and datasets, which can be learned by neural networks, given proper representations that are compatible with neural architectures and training approaches. Adopting the Deep Learning approach will significantly reduce the cost of game development by switching a large portion of the technical procedures of generating 3D shapes and scenes to pre-trained models, which in turn simplifies the game design process.
