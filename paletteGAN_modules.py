# tf.keras.backend.set_floatx('float64')
# import matplotlib.pyplot as plt
# import matplotlib.gridspec as gridspec
import os

from paletteGAN_data_loader import *
from paletteGAN_utils import *


class Generator(tf.keras.Model):
    def __init__(self, c_dim):
        super(Generator, self).__init__()
        # self.leaky_relu = tf.keras.layers.LeakyReLU(0.01)
        self.model = tf.keras.Sequential()
        self.model.add(tf.keras.layers.Dense(100, input_shape=(c_dim,), activation="relu"))
        self.model.add(tf.keras.layers.Dense(100, activation="relu"))
        self.model.add(tf.keras.layers.Dense(100, activation="relu"))
        self.model.add(tf.keras.layers.Dense(15, activation="sigmoid"))

    @tf.function
    def call(self, c: tf.Tensor) -> tf.Tensor:
        """Generates a batch of palettes given a tensor of class conditioning vectors.
        Inputs:
        - x: A [batch_size, input_sz] tensor of noise vectors
        Returns:
        TensorFlow Tensor with shape [batch_size, 15], containing the generated colors.
        """
        return self.model(c)


class Discriminator(tf.keras.Model):

    def __init__(self, palette_dim, c_dim):
        super(Discriminator, self).__init__()
        self.leaky_relu = tf.keras.layers.LeakyReLU(0.01)
        self.model = tf.keras.Sequential()
        self.model.add(tf.keras.layers.Dense(115, input_shape=(palette_dim + c_dim,), activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(115, activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(64, activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(1, activation="sigmoid"))

    @tf.function
    def call(self, palette: tf.Tensor, c: tf.Tensor) -> tf.Tensor:
        """Compute discriminator score for a batch of input palette and class condition.
        Inputs:
        - palette: TensorFlow Tensor of shape [batch_size, palette_dim], where palette_dim defaults to 15
        - c: TensorFlow Tensor of class condition of shape [batch_size, c_dim], where c_dim defaults to 300
        Returns:
        TensorFlow Tensor with shape [batch_size, 1], containing the score for a palette being real for
        each input (palette + class condition)."""
        assert palette.shape[0] == c.shape[0], "Palette and class condition must be of the same batch size."
        x = tf.concat([palette, c], axis=1)
        return self.model(x)

class PaletteGAN():
    def __init__(self, args):
        self.args = args
        """self.encoder = tf.keras.layers.Embedding(1, args["c_dim"],
                                                     embeddings_initializer=tf.keras.initializers.Constant(args["W_emb"]),
                                                     trainable=False)"""
        self.embeddings = args["W_emb"]
        self.generator = Generator(args["c_dim"])  # generator takes in "c" which is 1x300
        self.discriminator = Discriminator(args["palette_dim"], args["c_dim"])
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=args["lr"], beta_1=args["beta_1"])

    def generator_loss(self, logits_fake, fake_palettes, real_palettes) -> tf.Tensor:
        """Compute the discriminator loss.
        Inputs:
        - logits_fake: Tensor, shape[batch_size, 1], output of discriminator for each fake palette.
        - fake_palettes: Tensor, shape[batch_size, 15], generated by generator.
        - real_palettes: Tensor, shape[batch_size, 15], taken from training dataset."""
        G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_fake),
                                                                        logits=logits_fake))
        huber_critic = tf.keras.losses.Huber()
        smooth_l1_loss = huber_critic(fake_palettes, real_palettes)
        G_loss += (smooth_l1_loss * self.args["lambda_sL1"])

        return G_loss

    def discriminator_loss(self, logits_fake, logits_real) -> tf.Tensor:
        """Compute the discriminator loss."""
        D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            labels=tf.zeros_like(logits_fake),
            logits=logits_fake))
        D_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            labels=tf.ones_like(logits_real),
            logits=logits_real))
        return D_loss

    def optimize(self, tape: tf.GradientTape, component: tf.keras.Model, loss: tf.Tensor):
        """ This optimizes a component (generator or discriminator) with respect to its loss.
        Inputs:
        - tape: the Gradient Tape
        - model: the model to be trained
        - loss: the model's loss."""
        with tape as tape:
            gradients = tape.gradient(loss, component.trainable_variables)
            self.optimizer.apply_gradients(zip(gradients, component.trainable_variables))


def train(model, train_loader, num_epochs=1000):

    print('Start training...')
    g_loss_history = np.zeros(num_epochs)
    d_loss_history = np.zeros(num_epochs)
    for epoch in range(num_epochs):

        # Keep track of per palette (i.e. example) loss
        epoch_g_loss = []
        epoch_d_loss = []
        for batch_idx, (word_ids, real_palettes) in enumerate(train_loader):
            batch_size = word_ids.shape[0]

            with tf.GradientTape(persistent=True) as tape:
                # call, get outputs
                # print("word_ids shape:", word_ids.shape)
                # txt_embeddings = model.encoder(word_ids)
                # txt_embeddings = model.encoder(word_ids)
                txt_embeddings = tf.nn.embedding_lookup(model.embeddings, word_ids)
                fake_palettes = model.generator(txt_embeddings)
                # print("fake:", fake_palettes.shape)
                # print("txt", txt_embeddings)
                logits_real = model.discriminator(real_palettes, txt_embeddings)
                logits_fake = model.discriminator(fake_palettes, txt_embeddings)

                g_loss = model.generator_loss(logits_fake, fake_palettes, real_palettes)
                d_loss = model.discriminator_loss(logits_fake, logits_real)

            model.optimize(tape, model.generator, g_loss)
            model.optimize(tape, model.discriminator, d_loss)

            epoch_g_loss.append(g_loss/batch_size)
            epoch_d_loss.append(d_loss/batch_size)

        avg_g_loss = tf.reduce_mean(epoch_g_loss)
        avg_d_loss = tf.reduce_mean(epoch_d_loss)
        g_loss_history[epoch] = avg_g_loss
        d_loss_history[epoch] = avg_d_loss

        if epoch % 10 == 0 or epoch == num_epochs - 1:
            print("Epoch", epoch,
                  "generator loss:", avg_g_loss.numpy(), ", discriminator loss:", avg_d_loss.numpy())

    return model, g_loss_history, d_loss_history


def test(model, input_word):
    """
    Generate palettes from the user-input word.
    :param model: Trained PaletteGAN model
    :return: None
    """
    pass


def main():
    args = {
        "palette_dim": 15,
        "c_dim": 100,
        "lr": 1e-4,
        "lambda_sL1": 100,
        "beta_1": 0.5,
        "batch_size": 32,
        "num_epochs": 200
    }
    embed_file_path = os.path.join('./data', 'glove.6B.100d.txt')
    train_dataset, input_dict = t2p_loader(batch_size=args["batch_size"])
    args["W_emb"] = load_pretrained_embedding(dictionary=input_dict.word2index,
                                              embed_file=embed_file_path,
                                              embed_dim=args["c_dim"])
    args["n_words"] = len(input_dict.word2index)
    print("n_words:", len(input_dict.word2index))

    model = PaletteGAN(args=args)
    train(model=model, train_loader=train_dataset, num_epochs=args["num_epochs"])
    print("Training done!")

    return model
