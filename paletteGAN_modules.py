# tf.keras.backend.set_floatx('float64')
# import matplotlib.pyplot as plt
# import matplotlib.gridspec as gridspec
import os

from paletteGAN_data_loader import *
from paletteGAN_utils import *


class Generator(tf.keras.Model):
    def __init__(self, c_dim, hidden_dim, embeddings):
        super(Generator, self).__init__()
        self.embeddings = embeddings
        self.leaky_relu = tf.keras.layers.LeakyReLU(0.01)
        self.c_dim = c_dim

        self.encoder = tf.keras.Sequential()
        self.encoder.add(tf.keras.layers.Dense(c_dim*2, input_shape=(c_dim,), activation="relu"))

        self.model = tf.keras.Sequential()
        self.model.add(tf.keras.layers.Dense(hidden_dim, input_shape=(c_dim,), activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(hidden_dim, activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(hidden_dim, activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(15, activation="relu"))


    def reparametrize(self, mu, logvar):
        epsilon = tf.random.normal(shape=mu.shape)
        z = mu + (tf.exp(logvar * 0.5) * epsilon)
        return z


    @tf.function
    def call(self, word_ids: tf.Tensor, given_txt_embeddings=None):
        """Generates a batch of palettes given a tensor of class conditioning vectors.
        Inputs:
        - word_ids: A [batch_size, 1] tensor of word ids
        Returns:
        TensorFlow Tensor with shape [batch_size, 15], containing the generated colors.
        """
        if given_txt_embeddings == None:
            txt_embeddings = tf.nn.embedding_lookup(self.embeddings, word_ids)
        else:
            txt_embeddings = given_txt_embeddings

        x = self.encoder(txt_embeddings)
        mu = x[:, :self.c_dim]
        logvar = x[:, self.c_dim:]
        c = self.reparametrize(mu, logvar)
        gen_output = self.model(c)

        return gen_output, c, mu, logvar


class Discriminator(tf.keras.Model):

    def __init__(self, palette_dim, c_dim, hidden_dim):
        super(Discriminator, self).__init__()
        self.leaky_relu = tf.keras.layers.LeakyReLU(0.01)
        self.model = tf.keras.Sequential()
        self.model.add(tf.keras.layers.Dense(hidden_dim, input_shape=(palette_dim + c_dim,), activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(hidden_dim, activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(hidden_dim, activation=self.leaky_relu))
        self.model.add(tf.keras.layers.Dense(1))

    @tf.function
    def call(self, palette: tf.Tensor, c: tf.Tensor) -> tf.Tensor:
        """Compute discriminator score for a batch of input palette and class condition.
        Inputs:
        - palette: TensorFlow Tensor of shape [batch_size, palette_dim], where palette_dim defaults to 15
        - c: TensorFlow Tensor of class condition of shape [batch_size, c_dim], where c_dim defaults to 300
        Returns:
        TensorFlow Tensor with shape [batch_size, 1], containing the score for a palette being real for
        each input (palette + class condition)."""
        assert palette.shape[0] == c.shape[0], "Palette and class condition must be of the same batch size."
        x = tf.concat([palette, c], axis=1)
        return self.model(x)

class PaletteGAN():
    def __init__(self, args):
        self.args = args
        """self.encoder = tf.keras.layers.Embedding(1, args["c_dim"],
                                                     embeddings_initializer=tf.keras.initializers.Constant(args["W_emb"]),
                                                     trainable=False)"""
        self.generator = Generator(args["c_dim"], args["g_hidden_dim"], args["W_emb"])  # generator takes in "c" which is 1x300
        self.discriminator = Discriminator(args["palette_dim"], args["c_dim"], args["d_hidden_dim"])
        self.optimizer = tf.keras.optimizers.Adam(learning_rate=args["lr"], beta_1=args["beta_1"])

    def generator_loss(self, logits_fake, fake_palettes, real_palettes, mu, logvar) -> tf.Tensor:
        """Compute the discriminator loss.
        Inputs:
        - logits_fake: Tensor, shape[batch_size, 1], output of discriminator for each fake palette.
        - fake_palettes: Tensor, shape[batch_size, 15], generated by generator.
        - real_palettes: Tensor, shape[batch_size, 15], taken from training dataset."""
        GAN_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=tf.ones_like(logits_fake),
                                                                        logits=logits_fake))
        huber_critic = tf.keras.losses.Huber()
        smooth_l1_loss = huber_critic(fake_palettes, real_palettes)
        kl_loss = KL_loss(mu, logvar)
        G_loss = (GAN_loss * self.args["lambda_GAN"]) + \
                 (smooth_l1_loss * self.args["lambda_sL1"]) + \
                 (kl_loss * self.args["lambda_KL"])
        return G_loss

    def discriminator_loss(self, logits_fake, logits_real) -> tf.Tensor:
        """Compute the discriminator loss."""
        D_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            labels=tf.zeros_like(logits_fake),
            logits=logits_fake))
        D_loss += tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(
            labels=tf.ones_like(logits_real),
            logits=logits_real))
        return D_loss

    def optimize(self, tape: tf.GradientTape, component: tf.keras.Model, loss: tf.Tensor):
        """ This optimizes a component (generator or discriminator) with respect to its loss.
        Inputs:
        - tape: the Gradient Tape
        - model: the model to be trained
        - loss: the model's loss."""
        with tape as tape:
            gradients = tape.gradient(loss, component.trainable_variables)
            self.optimizer.apply_gradients(zip(gradients, component.trainable_variables))



def train(model, train_loader, num_epochs=1000):

    print('Start training...')
    g_loss_history = np.zeros(num_epochs)
    d_loss_history = np.zeros(num_epochs)
    for epoch in range(num_epochs):

        # Keep track of per palette (i.e. example) loss
        epoch_g_loss = []
        epoch_d_loss = []
        for batch_idx, (word_ids, real_palettes) in enumerate(train_loader):
            batch_size = word_ids.shape[0]

            with tf.GradientTape(persistent=True) as tape:
                # call, get outputs
                # print("word_ids shape:", word_ids.shape)
                fake_palettes, encoder_output, mu, logvar = model.generator(word_ids)
                # print("fake:", fake_palettes.shape)
                # print("txt", txt_embeddings)
                logits_real = model.discriminator(real_palettes, encoder_output) # txt_embeddings)
                logits_fake = model.discriminator(fake_palettes, encoder_output) # txt_embeddings)

                g_loss = model.generator_loss(logits_fake, fake_palettes, real_palettes, mu, logvar)
                d_loss = model.discriminator_loss(logits_fake, logits_real)

            model.optimize(tape, model.generator, g_loss)
            model.optimize(tape, model.discriminator, d_loss)

            epoch_g_loss.append(g_loss/batch_size)
            epoch_d_loss.append(d_loss/batch_size)

        avg_g_loss = tf.reduce_mean(epoch_g_loss)
        avg_d_loss = tf.reduce_mean(epoch_d_loss)
        g_loss_history[epoch] = avg_g_loss
        d_loss_history[epoch] = avg_d_loss

        if epoch % 10 == 0 or epoch == num_epochs - 1:
            print("Epoch", epoch,
                  "generator loss:", avg_g_loss.numpy(), ", discriminator loss:", avg_d_loss.numpy())

    return model, g_loss_history, d_loss_history


def display_palette(GAN_output, word: str, save_to_local=True, set_title=True):
    """
    Display the generated palette along with the input word.
    :param GAN_output: an Numpy array of 1x15, containing rgb values for the 5 colors but normalized to 0-1.
    :param word: the input to generator. Used here as the title for generated plot.
    :return: none.
    """
    GAN_output = GAN_output.reshape(5, 3)
    I = np.array([[0, 1, 2, 3, 4]])
    RGB = GAN_output[I]
    plt.imshow(RGB)
    if set_title:
        plt.title(word)
    if save_to_local:
        plt.savefig(os.path.join('./paletteGAN_outputs', word+'.png'))


def word_emb_direct_lookup(word: str, file_path:str) -> tf.Tensor:
    with open(file_path, "r") as f:
        for line in f.readlines():
            k, *v = line.strip().split(" ")
            if k == word:
                embedding = [float(num) for num in v]
                return tf.convert_to_tensor(embedding)
        raise Exception("Word not found in Glove embeddings.")


def test_run(model, inputs, color_scaling=1):
    """
    Generate palettes from the user-input word.
    :param model: Trained PaletteGAN model
    :param inputs: list of words
    :return: None
    """
    print('Fetching your colour recommendation...')
    #train_dataset, input_dict = t2p_loader(batch_size=32)
    recommendations = []

    for i in range(len(inputs)):
        word = inputs[i]
        plt.figure(i+1)

        """# dictionary = dict.fromkeys(inputs, word)
        word_ids = input_dict.word2index[word]
        txt_embeddings = tf.nn.embedding_lookup(model.embeddings, [word_id]) ## CHANGE THIS"""
        given_txt_embedding = word_emb_direct_lookup(word, model.args["embed_file_path"])
        given_txt_embedding = tf.expand_dims(given_txt_embedding, 0)
        colour_recommendation, a1, a2, a3 = model.generator(tf.zeros(1), given_txt_embedding)
        c = colour_recommendation.numpy() * color_scaling
        c = c.astype(int)
        print(word, "color rec:", c)

        display_palette(c, word, set_title=True, save_to_local=False)
        recommendations.append((word, c))
    return recommendations


def main():
    args = {
        "palette_dim": 15,
        "c_dim": 100,
        "g_hidden_dim": 256,
        "d_hidden_dim": 256,
        "lr": 5e-4,
        "lambda_GAN": 0.1,
        "lambda_sL1": 100,
        "lambda_KL": 0.5,
        "beta_1": 0.5,
        "batch_size": 32,
        "num_epochs": 300,
        "embed_file_path": os.path.join('./data', 'glove.6B.100d.txt')
    }
    train_dataset, input_dict = t2p_loader(batch_size=args["batch_size"])
    args["W_emb"] = load_pretrained_embedding(dictionary=input_dict.word2index,
                                              embed_file=args["embed_file_path"],
                                              embed_dim=args["c_dim"])
    args["n_words"] = len(input_dict.word2index)
    print("n_words:", len(input_dict.word2index))

    model = PaletteGAN(args=args)
    train(model=model, train_loader=train_dataset, num_epochs=args["num_epochs"])
    print("Training done!")

    return model
